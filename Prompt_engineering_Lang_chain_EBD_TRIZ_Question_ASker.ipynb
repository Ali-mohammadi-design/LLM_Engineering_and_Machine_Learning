{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPewwldZ2TiWkkMDSa+qIqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-mohammadi-design/Prompt_Engineering_and_Machine_Learning/blob/main/Prompt_engineering_Lang_chain_EBD_TRIZ_Question_ASker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this practice we want to recieve a sentence and ask questions regarding that sentence to gather information"
      ],
      "metadata": {
        "id": "pO8mRmGuhJ8y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DB2_KZ-JkT1",
        "outputId": "e20ade37-8f34-4ae3-f99b-ee6aadea0b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.6-py3-none-any.whl (811 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.8/811.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.18 (from langchain)\n",
            "  Downloading langchain_community-0.0.19-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.22 (from langchain)\n",
            "  Downloading langchain_core-0.1.22-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.90-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.6 langchain-community-0.0.19 langchain-core-0.1.22 langsmith-0.0.87 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.12.0\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=0b62a637c3b7c2625ba1b8479bab360b421ed3141e5471a81afac13abd737945\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install wikipedia\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain"
      ],
      "metadata": {
        "id": "VOKvAwVDJnNk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "openAI_API_Key=os.environ['openAI_API_Key']"
      ],
      "metadata": {
        "id": "JwryogXpJvaC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm= ChatOpenAI(openai_api_key=openAI_API_Key,temperature=0)\n",
        "llm2= ChatOpenAI(openai_api_key=openAI_API_Key)"
      ],
      "metadata": {
        "id": "LakjGpvUJzFr"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_template1=\" You are an AI assistant that gets text and return nouns. Please recognize verbs but do not return verbs or propositions\"\n",
        "system_message_prompt1= SystemMessagePromptTemplate.from_template(system_template1)\n",
        "human_template1='please list all the nouns in the following text: \\n {text}'\n",
        "human_message_prompt1=HumanMessagePromptTemplate.from_template(human_template1)\n",
        "prompt1=ChatPromptTemplate.from_messages([system_message_prompt1,human_message_prompt1])\n",
        "chain1=LLMChain(llm=llm2, prompt=prompt1, output_key='nouns')"
      ],
      "metadata": {
        "id": "nDUbcZOcJ1lV"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template2='Consider the following list of nouns. please specify which nouns are human and which nouns are non-human.These are the nouns: {nouns}'\n",
        "prompt2=ChatPromptTemplate.from_template(human_template2)\n",
        "chain2=LLMChain(llm=llm, prompt=prompt2, output_key='type')"
      ],
      "metadata": {
        "id": "IlhLNqboR98Q"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_template3=\" you are an AI assistant that get nouns and  their type and ask question with what or who. for the human nouns you ask: who is + noun? \\n For non-human nouns you ask: what is + noun?\"\n",
        "system_message_prompt3= SystemMessagePromptTemplate.from_template(system_template3)\n",
        "human_template3='''please consider the folowing nouns. for each noun you must ask questions with what or who depends on their type.\n",
        "for example if the nouns are teacher and sandwich you shoud return who is teacher? and what is sandwich? The nouns are : \\n {type}'''\n",
        "human_message_prompt3=HumanMessagePromptTemplate.from_template(human_template3)\n",
        "noun1='Bob, school, sandwich'\n",
        "example_one= HumanMessagePromptTemplate.from_template(noun1)\n",
        "question1='who is Bob? \\n what is school? \\n what is sandwich?'\n",
        "example_one_output= AIMessagePromptTemplate.from_template(question1)\n",
        "prompt3=ChatPromptTemplate.from_messages([system_message_prompt3,human_message_prompt3])\n",
        "chain3=LLMChain(llm=llm, prompt=prompt3, output_key='questions')"
      ],
      "metadata": {
        "id": "NwrKpEx4KShu"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='a cafe that people could study there'\n",
        "seq_chain=SequentialChain(chains=[chain1,chain2,chain3], input_variables=['text'],output_variables=['nouns','type','questions'],verbose=True)"
      ],
      "metadata": {
        "id": "z9-LMXquKSeN"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=seq_chain(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScpMVSNhM82y",
        "outputId": "89702729-eb05-49de-d957-e55886e48fc3"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Aqfsv8JN_Sq",
        "outputId": "c72a8c44-efb5-4b80-b4a0-b7a2eb07a90d"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'a cafe that people could study there',\n",
              " 'nouns': 'cafe, people, study',\n",
              " 'type': 'Cafe - non-human\\nPeople - human\\nStudy - non-human',\n",
              " 'questions': 'What is cafe?\\nWho is people?\\nWhat is study?'}"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_template0=''' \"You are an AI assistant who aims to help designers to extract as much information as possible from a design problem.\n",
        "You get a design problem as a sentence and ask many questions.\n",
        "'''\n",
        "system_message_prompt0= SystemMessagePromptTemplate.from_template(system_template0)\n",
        "human_template0='''' \"I'll provide a sentence. Please examine the given sentence, first identifying its verbs.\n",
        "After that, take note of all the verbs you've identified and generate questions using 'where, why, who, how, when' based on those verbs.\n",
        "The sentence is: {text}\"\n",
        "'''\n",
        "human_message_prompt0=HumanMessagePromptTemplate.from_template(human_template0)\n",
        "prompt0=ChatPromptTemplate.from_messages([system_template0,human_message_prompt0])\n",
        "request=prompt0.format_prompt(text=text).to_messages()\n",
        "result=llm(request)\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYleZXUkOBln",
        "outputId": "1f390882-e515-400c-861c-1fd3dfec6194"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verbs identified in the sentence: could, study\n",
            "\n",
            "Questions generated based on the verbs:\n",
            "1. Where could people study?\n",
            "2. Why would people study at the cafe?\n",
            "3. Who could study at the cafe?\n",
            "4. How could people study at the cafe?\n",
            "5. When could people study at the cafe?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As a function to get the sentence and ask question**"
      ],
      "metadata": {
        "id": "1tIBtOg9hTLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain"
      ],
      "metadata": {
        "id": "6xy70Yd_hlT3"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "openAI_API_Key=os.environ['openAI_API_Key']"
      ],
      "metadata": {
        "id": "oQmecAX_hoEL"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm= ChatOpenAI(openai_api_key=openAI_API_Key,temperature=0)"
      ],
      "metadata": {
        "id": "F-qgJ__8hr98"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def question_asking(sentence):\n",
        "  system_template1=\" You are an AI assistant that gets text and return nouns. Please recognize verbs but do not return verbs or propositions\"\n",
        "  system_message_prompt1= SystemMessagePromptTemplate.from_template(system_template1)\n",
        "  human_template1='please list all the nouns in the following text: \\n {text}'\n",
        "  human_message_prompt1=HumanMessagePromptTemplate.from_template(human_template1)\n",
        "  prompt1=ChatPromptTemplate.from_messages([system_message_prompt1,human_message_prompt1])\n",
        "  chain1=LLMChain(llm=llm, prompt=prompt1, output_key='nouns')\n",
        "\n",
        "  human_template2='Consider the following list of nouns. please specify which nouns are human and which nouns are non-human.These are the nouns: {nouns}'\n",
        "  prompt2=ChatPromptTemplate.from_template(human_template2)\n",
        "  chain2=LLMChain(llm=llm, prompt=prompt2, output_key='type')\n",
        "\n",
        "  system_template3=\" you are an AI assistant that get nouns and  their type and ask question with what or who. for the human nouns you ask: who is + noun? \\n For non-human nouns you ask: what is + noun?\"\n",
        "  system_message_prompt3= SystemMessagePromptTemplate.from_template(system_template3)\n",
        "  human_template3='''please consider the folowing nouns. for each noun you must ask questions with what or who depends on their type.\n",
        "  for example if the nouns are teacher and sandwich you shoud return who is teacher? and what is sandwich? The nouns are : \\n {type}'''\n",
        "  human_message_prompt3=HumanMessagePromptTemplate.from_template(human_template3)\n",
        "  noun1='Bob, school, sandwich'\n",
        "  example_one= HumanMessagePromptTemplate.from_template(noun1)\n",
        "  question1='who is Bob? \\n what is school? \\n what is sandwich?'\n",
        "  example_one_output= AIMessagePromptTemplate.from_template(question1)\n",
        "  prompt3=ChatPromptTemplate.from_messages([system_message_prompt3,human_message_prompt3])\n",
        "  chain3=LLMChain(llm=llm, prompt=prompt3, output_key='questions')\n",
        "\n",
        "  text=sentence\n",
        "  seq_chain=SequentialChain(chains=[chain1,chain2,chain3], input_variables=['text'],output_variables=['nouns','type','questions'],verbose=True)\n",
        "  result1=seq_chain(text)\n",
        "  noun_questions=result1\n",
        "\n",
        "\n",
        "  system_template0=''' \"You are an AI assistant who aims to help designers to extract as much information as possible from a design problem.\n",
        "  You get a design problem as a sentence and ask questions from each verb that you recognize in the sentence.\n",
        "  '''\n",
        "  system_message_prompt0= SystemMessagePromptTemplate.from_template(system_template0)\n",
        "  human_template0='''' \"I'll provide a sentence for you. Please examine the given sentence, first identifying its verbs.\n",
        "  After that, take note of each verb you've identified in the provided sentence and generate questions for each verb using 'where, why, who, how, when' based on those verbs.\n",
        "  \\n The sentence is: \\n {text}\"\n",
        "  '''\n",
        "  human_message_prompt0=HumanMessagePromptTemplate.from_template(human_template0)\n",
        "  prompt0=ChatPromptTemplate.from_messages([system_message_prompt0,human_message_prompt0])\n",
        "  request=prompt0.format_prompt(text=text).to_messages()\n",
        "  result0=llm(request)\n",
        "  verb_questions=result0\n",
        "\n",
        "  return print(f\"the noun questions are: \\n{noun_questions['questions']} \\n\\n\\n the verb questions are:\\n \\n{verb_questions.content}\")"
      ],
      "metadata": {
        "id": "EEzoi5wsgO9s"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_asking('design a house that can fly easily from one loaction to another location')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfv_t493i4Hp",
        "outputId": "adfc0ca6-15d2-4ced-819a-5289427392a5"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "the noun questions are: \n",
            "What is house?\n",
            "Who is location? \n",
            "\n",
            "\n",
            " the verb questions are:\n",
            " \n",
            "Verbs identified in the sentence: design, fly\n",
            "\n",
            "Questions for the verb 'design':\n",
            "1. Where should the house be designed?\n",
            "2. Why is the house being designed?\n",
            "3. Who is responsible for designing the house?\n",
            "4. How should the house be designed?\n",
            "5. When should the house be designed?\n",
            "\n",
            "Questions for the verb 'fly':\n",
            "1. Where should the house be able to fly?\n",
            "2. Why should the house be able to fly?\n",
            "3. Who will operate the house while flying?\n",
            "4. How will the house fly?\n",
            "5. When will the house need to fly?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39ghNcNZi_Rj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}